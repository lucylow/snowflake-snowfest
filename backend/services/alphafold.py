import subprocess
import tempfile
import os
import hashlib
from pathlib import Path
from typing import Optional, Tuple
import aiofiles
import logging

logger = logging.getLogger(__name__)

ALPHAFOLD_IMAGE = os.getenv("ALPHAFOLD_DOCKER_IMAGE", "alphafold")
ALPHAFOLD_DATA_DIR = os.getenv("ALPHAFOLD_DATA_DIR", "/data/alphafold")
USE_CLOUD_API = os.getenv("ALPHAFOLD_USE_CLOUD_API", "false").lower() == "true"

async def run_alphafold(sequence: str, job_id: str) -> Tuple[Path, float]:
    """
    Run AlphaFold structure prediction on a protein sequence.
    
    Args:
        sequence: Amino acid sequence (FASTA format)
        job_id: Unique job identifier
        
    Returns:
        Tuple of (predicted_pdb_path, plddt_confidence_score)
    """
    # Check cache first
    cached_result = await get_cached_structure(sequence)
    if cached_result:
        logger.info(f"Using cached structure for job {job_id}")
        return cached_result
    
    # Choose between local Docker or cloud API
    if USE_CLOUD_API:
        return await run_alphafold_cloud(sequence, job_id)
    else:
        return await run_alphafold_docker(sequence, job_id)

async def run_alphafold_docker(sequence: str, job_id: str) -> Tuple[Path, float]:
    """Run AlphaFold using local Docker container"""
    output_dir = Path(f"/workspace/predictions/{job_id}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Create temporary FASTA file
    fasta_content = f">target\n{sequence}"
    fasta_path = output_dir / "input.fasta"
    
    async with aiofiles.open(fasta_path, 'w') as f:
        await f.write(fasta_content)
    
    try:
        # Run AlphaFold via Docker
        cmd = [
            "docker", "run", "--gpus", "all", "--rm",
            "-v", f"{ALPHAFOLD_DATA_DIR}:/data",
            "-v", f"{output_dir}:/output",
            "-v", f"{fasta_path}:/input.fasta",
            ALPHAFOLD_IMAGE,
            "--fasta_paths=/input.fasta",
            "--max_template_date=2024-01-01",
            "--db_preset=reduced_dbs",
            "--model_preset=monomer",
            "--data_dir=/data",
            "--output_dir=/output",
            "--use_gpu_relax=true"
        ]
        
        logger.info(f"Running AlphaFold for job {job_id}")
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "Unknown error"
            logger.error(f"AlphaFold failed for job {job_id}: {error_msg}")
            raise RuntimeError(f"AlphaFold prediction failed: {error_msg}")
        
        # Find the best predicted PDB file (ranked_0.pdb)
        predicted_pdb = output_dir / "ranked_0.pdb"
        if not predicted_pdb.exists():
            # Fallback to any .pdb file
            pdb_files = list(output_dir.glob("*.pdb"))
            if not pdb_files:
                raise RuntimeError("No PDB file generated by AlphaFold")
            predicted_pdb = pdb_files[0]
        
        # Extract pLDDT score from result
        plddt_score = await extract_plddt_score(output_dir)
        
        # Cache the result
        await cache_structure(sequence, predicted_pdb, plddt_score)
        
        logger.info(f"AlphaFold completed for job {job_id}, pLDDT: {plddt_score:.2f}")
        return predicted_pdb, plddt_score
        
    except Exception as e:
        logger.error(f"Error running AlphaFold for job {job_id}: {str(e)}")
        raise

async def run_alphafold_cloud(sequence: str, job_id: str) -> Tuple[Path, float]:
    """Run AlphaFold using NVIDIA BioNeMo API via Snowflake"""
    import httpx
    
    api_key = os.getenv("BIONEMO_API_KEY")
    if not api_key:
        raise ValueError("BIONEMO_API_KEY not set for cloud API")
    
    output_dir = Path(f"/workspace/predictions/{job_id}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"Submitting job {job_id} to BioNeMo Cloud API")
    
    async with httpx.AsyncClient(timeout=300.0) as client:
        # Submit prediction request
        response = await client.post(
            "https://api.bionemo.nvidia.com/v1/protein/structure/predict",
            headers={"Authorization": f"Bearer {api_key}"},
            json={
                "sequence": sequence,
                "model": "alphafold2",
                "output_format": "pdb",
                "include_confidence": True
            }
        )
        
        if response.status_code != 200:
            raise RuntimeError(f"BioNeMo API error: {response.text}")
        
        result = response.json()
        
        # Download predicted structure
        pdb_url = result.get("pdb_url")
        if not pdb_url:
            raise RuntimeError("No PDB URL returned from API")
        
        pdb_response = await client.get(pdb_url)
        predicted_pdb = output_dir / "predicted_structure.pdb"
        
        async with aiofiles.open(predicted_pdb, 'wb') as f:
            await f.write(pdb_response.content)
        
        # Extract confidence score
        plddt_score = result.get("plddt_score", 0.0)
        
        # Cache the result
        await cache_structure(sequence, predicted_pdb, plddt_score)
        
        logger.info(f"BioNeMo prediction completed for job {job_id}, pLDDT: {plddt_score:.2f}")
        return predicted_pdb, plddt_score

async def extract_plddt_score(output_dir: Path) -> float:
    """Extract average pLDDT confidence score from AlphaFold output"""
    # Look for ranking_debug.json which contains pLDDT scores
    ranking_file = output_dir / "ranking_debug.json"
    
    if ranking_file.exists():
        import json
        async with aiofiles.open(ranking_file, 'r') as f:
            content = await f.read()
            data = json.loads(content)
            # Get pLDDT for the top-ranked model
            if "plddts" in data and data["plddts"]:
                return float(data["plddts"]["ranked_0"])
    
    # Fallback: parse from PDB file B-factors (which store pLDDT)
    pdb_file = output_dir / "ranked_0.pdb"
    if pdb_file.exists():
        plddts = []
        async with aiofiles.open(pdb_file, 'r') as f:
            async for line in f:
                if line.startswith("ATOM"):
                    try:
                        plddt = float(line[60:66].strip())
                        plddts.append(plddt)
                    except ValueError:
                        continue
        
        if plddts:
            return sum(plddts) / len(plddts)
    
    # Default fallback
    return 0.0

async def get_cached_structure(sequence: str) -> Optional[Tuple[Path, float]]:
    """Check if structure prediction is cached"""
    seq_hash = hashlib.sha256(sequence.encode()).hexdigest()
    cache_dir = Path("/workspace/cache")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    cache_pdb = cache_dir / f"{seq_hash}.pdb"
    cache_meta = cache_dir / f"{seq_hash}.meta"
    
    if cache_pdb.exists() and cache_meta.exists():
        try:
            async with aiofiles.open(cache_meta, 'r') as f:
                import json
                content = await f.read()
                meta = json.loads(content)
                return cache_pdb, meta.get("plddt_score", 0.0)
        except Exception as e:
            logger.warning(f"Failed to read cache: {e}")
    
    return None

async def cache_structure(sequence: str, pdb_path: Path, plddt_score: float):
    """Cache structure prediction result"""
    seq_hash = hashlib.sha256(sequence.encode()).hexdigest()
    cache_dir = Path("/workspace/cache")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    cache_pdb = cache_dir / f"{seq_hash}.pdb"
    cache_meta = cache_dir / f"{seq_hash}.meta"
    
    try:
        # Copy PDB file
        async with aiofiles.open(pdb_path, 'rb') as src:
            content = await src.read()
        async with aiofiles.open(cache_pdb, 'wb') as dst:
            await dst.write(content)
        
        # Save metadata
        import json
        meta = {
            "sequence_hash": seq_hash,
            "plddt_score": plddt_score,
            "cached_at": str(datetime.now())
        }
        async with aiofiles.open(cache_meta, 'w') as f:
            await f.write(json.dumps(meta))
        
        logger.info(f"Cached structure with hash {seq_hash}")
    except Exception as e:
        logger.warning(f"Failed to cache structure: {e}")

import asyncio
from datetime import datetime
